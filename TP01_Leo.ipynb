{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "257a5916",
   "metadata": {},
   "source": [
    "# TP 01 - Optimization for Machine Learning\n",
    "\n",
    "### Gustavo Duarte Tomaz de Sá\n",
    "### Leonardo Hannas de Carvalho Santos\n",
    "\n",
    "**The report is presented in the python notebook itself. All the mathematical developments are presented in the report. The code is also commented when needed in order to explain the different steps of the algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6397c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "259e8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data_matrix_train, COP_train, data_matrix_test, COP_test, names = np.load('data_center_data_matrix.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72fa44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing matrices for min_w ||A w - b||_2**2\n",
    "\n",
    "matrix_mean = np.mean(data_matrix_train, axis=0)\n",
    "M = data_matrix_train - matrix_mean\n",
    "matrix_std = np.std(M, axis=0)\n",
    "M = M / matrix_std\n",
    "\n",
    "A = np.hstack([M, np.ones((M.shape[0],1)), -(M.T * COP_train[:,3]).T])\n",
    "b = COP_train[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6a3409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing matrices for the test set\n",
    "\n",
    "M_test = (data_matrix_test - matrix_mean) / matrix_std\n",
    "A_test = np.hstack([M_test, np.ones((M_test.shape[0],1)), -(M_test.T * COP_test[:,3]).T])\n",
    "b_test = COP_test[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fcb7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading raw data\n",
    "\n",
    "data = pd.read_csv('Raw_Dataset_May.csv')\n",
    "\n",
    "def name_to_subcategory_and_details(col_name):\n",
    "    if np.isreal(col_name):\n",
    "        col_name = names[col_name]\n",
    "    indices = np.nonzero((data['NAME'] == col_name).values)[0]\n",
    "    if len(indices) > 0:\n",
    "        subcategory = data['SUBCATEGORY'].iloc[[indices[0]]].values[0]\n",
    "        details = data['DETAILS'].iloc[[indices[0]]].values[0]\n",
    "        return subcategory, details\n",
    "    else:\n",
    "        print('unknown name')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e105f",
   "metadata": {},
   "source": [
    "# 3. Least Squares \n",
    "\n",
    "In order to fit this model to the data, we first standardize the data. Indeed, the measurements may have various units (like $\\mathrm{kWh}$, degree Celsius, $\\mathrm{V}$, etc) and it makes more sense to separate the statistical aspects from these dimensionality considerations. Hence, we consider the matrix $\\tilde{x}$, which is such that each of its columns has mean and standard deviation respectively 0 and 1 over the training set. (Note that we do not use the test set to do this standardization.)\n",
    "\n",
    "We then solve the following least squares problem\n",
    "\n",
    "$$\n",
    "\\min _{w} \\frac{1}{2}\\|A w-b\\|^{2}\n",
    "$$\n",
    "\n",
    "where $(A w)_{t}=\\tilde{x}(t)^{\\top} w_{1}+w_{0}-y(t) \\times \\tilde{x}(t)^{\\top} w_{2}$ for all $t$ and $b_{t}=y(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89969c3a",
   "metadata": {},
   "source": [
    "## Question 3.1\n",
    "\n",
    "**Show that if $A\\omega = b$, then $y(t) = \\frac{\\omega_{1}^{T}\\widetilde{x}(t) + \\omega_{0}}{\\omega_{2}^{T}\\widetilde{x}(t) + 1}$.**\n",
    "\n",
    "We know that the $t^{th}$ element of $A\\omega$, as well as the $t^{th}$ element of $b$ are given by:\n",
    "\n",
    "$$\n",
    "\\left\\{\\begin{matrix}\n",
    "(A\\omega)_{t} = \\widetilde{x}(t)^{T} \\omega_{1} + \\omega_{0} - y(t) \\times \\widetilde{x}(t)^{T} \\omega_{2}\n",
    "\\\\ \n",
    "b_{t} = y(t)\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "Then, it follows that, for every $t$, \n",
    "\n",
    "$$\n",
    "(A\\omega)_{t} = b_{t} \\iff\n",
    "\\widetilde{x}(t)^{T} \\omega_{1} + \\omega_{0} - y(t) \\times \\widetilde{x}(t)^{T} \\omega_{2} = y(t) \\iff\n",
    "\\widetilde{x}(t)^{T} \\omega_{1} + \\omega_{0} = y(t) \\times \\widetilde{x}(t)^{T} \\omega_{2} + y(t) \\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "y(t) = \\frac{\\widetilde{x}(t)^{T} \\omega_{1} + \\omega_{0}}{\\widetilde{x}(t)^{T} \\omega_{2} + 1} \\iff\n",
    "y(t) = \\frac{\\omega_{1}^{T} \\widetilde{x}(t) + \\omega_{0}}{\\omega_{2}^{T} \\widetilde{x}(t) + 1}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50128a",
   "metadata": {},
   "source": [
    "## Question 3.2\n",
    "\n",
    "Solve this least squares problem using the function `numpy.linalg.lstsq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c836d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(722, 1785)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_train = A\n",
    "A_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2337322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361, 1785)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44c49ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(722,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_train = b\n",
    "b_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7af59d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "472896fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1785"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model Aw = b\n",
    "w_train = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "w_train\n",
    "len(w_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efd9bc6",
   "metadata": {},
   "source": [
    "## Question 3.3\n",
    "Evaluate the quality of the solution found on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e82d2498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_b = A_test @ w_train\n",
    "new_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0ca93b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 780.8984793523309\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Root Mean Squared Error\n",
    "mse = mean_squared_error(new_b, b_test)\n",
    "print(f\"Root Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d5b68",
   "metadata": {},
   "source": [
    "## Question 3.4\n",
    "\n",
    "**In order to improve the generalization power of the model, we consider a $\\ell_{2}$ regularization :**\n",
    "\n",
    "\n",
    "$$\n",
    "\\min _{w} \\frac{1}{2}\\|A w-b\\|^{2}+\\frac{\\lambda}{2}\\|w\\|^{2}\n",
    "$$\n",
    "\n",
    "**where $\\lambda=100$. Solve this problem and compare the test mean square error with the unregularized one.**\n",
    "\n",
    "First, we calculate the gradient of the function. By definition:\n",
    "\n",
    "$$\n",
    "f(w+h) = \\frac{1}{2}\\|A(w + h) -b\\|^{2} + \\frac{\\lambda}{2}\\|w+h\\|^{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{2}\\|A w -b + A h\\|^{2} + \\frac{\\lambda}{2}\\|w+h\\|^{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{2}\\left\\|A w-b\\right\\|^{2}+\\left\\langle A w-b, A h\\right\\rangle+\\frac{1}{2}\\left\\|A h\\right\\|^{2}+\\frac{\\lambda}{2}\\left\\|w\\right\\|^{2}+\\lambda\\left\\langle w, h\\right\\rangle+\\frac{\\lambda}{2}\\left\\|h\\right\\|^{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= f_1(w) + \\left\\langle A w-b, A h\\right\\rangle + \\lambda\\left\\langle w, h\\right\\rangle + o_1(h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= f_1(w) + \\left\\langle A^{T}(A w-b) + \\lambda w, h\\right\\rangle + o_1(h)\n",
    "$$\n",
    "\n",
    "Since, the first element of the product is the gradient of $f_1(w)$, we have:\n",
    "\n",
    "$$\n",
    "\\nabla f(w) = A^{T}(A w-b) + \\lambda w\n",
    "$$\n",
    "\n",
    "After obtaining the gradient, taking it equal to 0 (and assuming the function to be convex) we will find the minimization solution vector w:\n",
    "\n",
    "$$\n",
    "\\nabla f(w) = 0 \\Rightarrow A^{T}(A w-b) + \\lambda w = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{T}A w + \\lambda w = A^{T}b\t\n",
    "$$\n",
    "\n",
    "$$\n",
    "(A^{T}A + \\lambda I)w = A^{T}b\n",
    "$$\n",
    "\n",
    "Then:\n",
    "$$\n",
    "w = (A^{T}A + \\lambda I)^{-1}A^{T}b\n",
    "$$\n",
    "\n",
    "With the solution vector w, we can calculate the w_regularized_train as we named, since we use the train set to define it.\n",
    "\n",
    "With this value in hand, we can calculate the b_regularized for test purposes, using the trainin set w that we calculated and the A matrix set. Finally, to compare we calculate mean squared error between the regularized model and the normal b_result, that we called new_b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9857946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (Regularized): 301.05482810267125\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 100\n",
    "w_regularized_train = np.linalg.inv(np.transpose(A_train) @ A_train + lambda_ * np.identity(A_train.shape[1])) @ np.transpose(A_train) @ b_train\n",
    "w_regularized_train\n",
    "\n",
    "new_b_regularized = A_test @ w_regularized_train\n",
    "\n",
    "# Comparing the errors\n",
    "mse = mean_squared_error(new_b_regularized, b_test)\n",
    "print(f\"Root Mean Squared Error (Regularized): {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3b9d8",
   "metadata": {},
   "source": [
    "## Question 3.5\n",
    "**Calculate the gradient of $f_{1}: w \\mapsto \\frac{1}{2}\\|A w-b\\|^{2}+\\frac{\\lambda}{2}\\|w\\|^{2}$. Is the function convex?**\n",
    "\n",
    "We first need to compute the Hessian Matrix of the function $f_{1}(\\omega)$. It has already been done in the Question 3.4.\n",
    "\n",
    "$$\n",
    "\\nabla^{2}f_{1}(\\omega) = \\nabla (\\nabla f_{1}(\\omega)) = \\nabla (A^{T} (A\\omega - b) + \\lambda \\omega) = \\nabla (A^{T}A\\omega - A^{T}b + \\lambda \\omega) = A^{T}A + \\lambda I\n",
    "$$\n",
    "\n",
    "Now we consider a non-zero vector $x$ and we analyse $A^{T}A$:\n",
    "\n",
    "$$\n",
    "x^{T}A^{T}Ax = (Ax)^{T}(Ax) = \\|Ax\\|^{2} \\geq 0\n",
    "$$\n",
    "\n",
    "Moreover, given that $\\lambda > 0$, then the matrix $\\lambda I$ is positive definite, because all its eigenvalues ate strictly positive. Therefore, the Hessian Matrix of $f_{1}(\\omega)$ is positive definite, which means that $f_{1}(\\omega)$ is strongly convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d2248e",
   "metadata": {},
   "source": [
    "## Question 3.6\n",
    "**Implement gradient descent to minimize $f_{1}$. What step size are you choosing? How many iterations are needed to get $w_{k}$ such that $\\left\\|\\nabla f\\left(w_{k}\\right)\\right\\| \\leq 1$ ?**\n",
    "\n",
    "From the Question 3.4, the gradient of the function $f_{1}$ is given by $\\nabla f_{1}(\\omega) = A^{T} (A \\omega - b) + \\lambda \\omega$. From this, we will derive the value of the step size $\\gamma = 1 / L$, where $L$ is the Lipschitz constant of the gradient of $f_{1}$.\n",
    "\n",
    "Let $\\omega_{1}$ and $\\omega_{2}$ be two vectors in $Dom(f_{1})$, with $\\omega_{1} \\neq \\omega_{2}$. Then, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\left \\| \\nabla f_{1}(\\omega_{1}) - \\nabla f_{1}(\\omega_{2}) \\right \\| &= \\left \\| A^{T} (A \\omega_{1} - b) + \\lambda \\omega_{1} - A^{T} (A \\omega_{2} - b) - \\lambda \\omega_{2} \\right \\| \\\\\n",
    "&= \\left \\| A^{T} A (\\omega_{1} - \\omega_{2}) + \\lambda (\\omega_{1} - \\omega_{2}) \\right \\| \\\\\n",
    "&= \\left \\| (A^{T} A + \\lambda I) (\\omega_{1} - \\omega_{2}) \\right \\| \\\\\n",
    "&\\leq \\left \\| A^{T} A + \\lambda I \\right \\| \\left \\| \\omega_{1} - \\omega_{2} \\right \\| \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Considering the Norm Operator $\\left \\| \\cdot \\right \\|$ of Linear Operator $T:V \\rightarrow W$, we retrieve the largest value by which $T$ stretches an element of $V$, obtaining the following equality:\n",
    "\n",
    "$$\n",
    "\\|T\\| = \\sup_{\\|v\\|=1} \\|T(v)\\|.\n",
    "$$\n",
    "\n",
    "Knowing that $A^{T}A + \\lambda I$ is symmetric, $ \\left \\| A^{T} A + \\lambda I \\right \\|$ is equal to the square root of the largest eigenvalue of $(A^{T}A + \\lambda I)^{T}(A^{T}A + \\lambda I)$. Therefore, we compute the expression of the L-Lipschitz constant:\n",
    "\n",
    "$$\n",
    "L = \\sqrt{\\lambda_{max}(B)}, \\text{ where } B = (A^{T}A + \\lambda I)^{T}(A^{T}A + \\lambda I)\n",
    "$$\n",
    "\n",
    "Finally, we set the step size $\\gamma = 1 / L$ and we implement the Gradient Descent Algorithm. We stop the algorithm when $\\left\\|\\nabla f\\left(w_{k}\\right)\\right\\| \\leq 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4853f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the function f1\n",
    "def f1(w, A, b, lambda_):\n",
    "    return 0.5 * np.linalg.norm(A @ w - b)**2 + 0.5 * lambda_ * np.linalg.norm(w)**2\n",
    "\n",
    "# Definition of the gradient of f1\n",
    "def grad_f1(w, A, b, lambda_):\n",
    "    return A.T @ (A @ w - b) + lambda_ * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5d6235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the weight vector w\n",
    "w = np.zeros(A_train.shape[1])\n",
    "\n",
    "# Auxiliary matrices\n",
    "C = A_train.T @ A_train + lambda_ * np.identity(A_train.shape[1])\n",
    "B = C.T @ C\n",
    "\n",
    "# L-Lipschitz constant of the gradient of f1\n",
    "L = np.sqrt(np.linalg.eigvals(B).max())\n",
    "\n",
    "# Step size gamma\n",
    "gamma = 1 / L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7a86fe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[1;32m     10\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad_f1(w, A_train, b_train, lambda_)\n\u001b[0;32m---> 11\u001b[0m     f1_val \u001b[38;5;241m=\u001b[39m \u001b[43mf1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     norm_grad_list\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(grad))\n\u001b[1;32m     14\u001b[0m     f1_list\u001b[38;5;241m.\u001b[39mappend(f1_val)\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mf1\u001b[0;34m(w, A, b, lambda_)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf1\u001b[39m(w, A, b, lambda_):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m \u001b[38;5;241m-\u001b[39m b)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m lambda_ \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(w)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Gradient Descent algorithm \n",
    "max_iter = 100000  # Maximum number of iterations\n",
    "tol = 1  # Tolerance for the norm of the gradient\n",
    "\n",
    "norm_grad_list = []  # List to store the values of the norm of the gradient\n",
    "f1_list = []  # List to store the values of f1\n",
    "\n",
    "for k in range(max_iter):\n",
    "    \n",
    "    grad = grad_f1(w, A_train, b_train, lambda_)\n",
    "    f1_val = f1(w, A_train, b_train, lambda_)\n",
    "    \n",
    "    norm_grad_list.append(np.linalg.norm(grad))\n",
    "    f1_list.append(f1_val)\n",
    "    \n",
    "    if np.linalg.norm(grad) <= tol:\n",
    "        break\n",
    "    \n",
    "    w = w - gamma * grad\n",
    "\n",
    "\n",
    "# Plots for f1_list and norm_grad_list\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 7), dpi=100)\n",
    "\n",
    "# Plot for f1_list on the first subplot\n",
    "ax1.plot(f1_list, label='f1 Value', color='blue', linewidth=2, linestyle='--')\n",
    "ax1.set_xlabel('Iteration', fontsize=14)\n",
    "ax1.set_ylabel('f1 Value', fontsize=14)\n",
    "ax1.set_title('Convergence of f1 Over Iterations', fontsize=16)\n",
    "ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "ax1.set_yscale('log')\n",
    "ax1.legend(fontsize=12)\n",
    "\n",
    "# Plot for norm_grad_list on the second subplot\n",
    "ax2.plot(norm_grad_list, label='Gradient Norm', color='red', linewidth=2, linestyle='-')\n",
    "ax2.set_xlabel('Iteration', fontsize=14)\n",
    "ax2.set_ylabel('Norm of Gradient', fontsize=14)\n",
    "ax2.set_title('Convergence of the Norm of Gradient Over Iterations', fontsize=16)\n",
    "ax2.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "ax2.set_yscale('log')\n",
    "ax2.legend(fontsize=12)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of iterations: {k}\")\n",
    "print(f\"Final norm of the gradient: {np.linalg.norm(grad)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef7cbf",
   "metadata": {},
   "source": [
    "# 4. Regularization for a sparse model\n",
    "\n",
    "You may have seen that at the optimum, the parameter $w$ has many coordinates with small but nonzero values. In order to force most of them to be exactly 0 and thus, to concentrate on the really informative features, we can solve a Lasso problem, that is a least squares problem with $\\ell_{1}$ regularization :\n",
    "\n",
    "$$\n",
    "\\min _{w} \\frac{1}{2}\\|A w-b\\|^{2}+\\lambda\\|w\\|_{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837657d7",
   "metadata": {},
   "source": [
    "## Question 4.1\n",
    "\n",
    "**Write the objective function as $F_{2}=f_{2}+g_{2}$ where $f_{2}$ is differentiable and the proximal operator of $g_{2}$ is easy to compute. Recall the formula for $\\operatorname{prox}_{g_{2}}$. Calculate the gradient of $f_{2}$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3b59e",
   "metadata": {},
   "source": [
    "$$\n",
    "min_{x \\in \\mathbb{R}^n}  \\frac{1}{2}\\|A w-b\\|^{2}+\\lambda\\|w\\|\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_i = prox_{g2}(w_i) = argmin_{w \\in \\mathbb{R}^n} |w| + \\frac{1}{2\\lambda} \\|w - w_i\\|^2\n",
    "$$\n",
    "\n",
    "Using Fermat's rule, we have:\n",
    "\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "\n",
    "$$\n",
    "0 \\in \\partial \\left[|.|(P_i) + \\frac{1}{2\\lambda}(. - w_i)^{2}(P_i)\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "0 \\in \\partial |.|(P_i) + \\frac{1}{\\lambda}(P_i - w_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\lambda} (w_i - P_i) \\in  \\partial |.|(P_i)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\lambda} (w_i - P_i) \\in  \n",
    "\\Biggl\\{\n",
    "\\begin{aligned}\n",
    "-1,  P_i < 0\\\\\n",
    "[-1,1],  P_i = 0\\\\\n",
    "1,  P_i > 0 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_i \\in  \n",
    "\\Biggl\\{\n",
    "\\begin{aligned}\n",
    "\\lambda + w_i,  P_i < 0\\\\\n",
    "w_i + [-\\lambda,\\lambda],  P_i = 0\\\\\n",
    "w_i - \\lambda ,  P_i > 0 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "If \\ P_i < 0 \\Rightarrow P_i = \\lambda + w_i < 0 \\Rightarrow w_i < -\\lambda\n",
    "$$\n",
    "\n",
    "$$\n",
    "If \\ P_i > 0 \\Rightarrow P_i = w_i - \\lambda > 0 \\Rightarrow w_i > \\lambda\n",
    "$$\n",
    "\n",
    "$$\n",
    "If \\ P_i = 0 \\Rightarrow P_i \\in [-\\lambda,\\lambda] \\Rightarrow |w_i| \\leq \\lambda\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ P_i \\in\n",
    "\\Biggl\\{\n",
    "\\begin{aligned}\n",
    "x_i + \\lambda, w_i < -\\lambda \\\\\n",
    "x_i - \\lambda, w_i > \\lambda \\\\\n",
    "0, |w_i| \\le \\lambda \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we have to calculate the gradient of the function f2:\n",
    "\n",
    "As $f$ is defined by:\n",
    "$$\n",
    "\\frac{1}{2}\\|A w-b\\|^{2}\n",
    "$$\n",
    "\n",
    "The gradient can be calculated by definition:\n",
    "\n",
    "$$\n",
    "f(w+h) = \\frac{1}{2}\\|A(w + h) -b\\|^{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{2}\\|A w -b + A h\\|^{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= f_1(w) + \\left\\langle A w-b, A h\\right\\rangle + o_1(h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= f_1(w) + \\left\\langle A^{T} (Aw-b), h\\right\\rangle + o_1(h)\n",
    "$$\n",
    "\n",
    "Finally:\n",
    "\n",
    "$$\n",
    "\\nabla f(w) = A^{T}(A w-b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977273e2",
   "metadata": {},
   "source": [
    "## Question 4.2\n",
    "Code the proximal gradient method. Here, we will take $\\lambda = 200$. What stopping test do you suggest ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e77a396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result :  [ 0.  0.  0. ... -0.  0.  0.]\n",
      "Optimal Value: 868.7013757510699\n"
     ]
    }
   ],
   "source": [
    "def soft_thresholding(x, lmbda):\n",
    "    return np.sign(x) * np.maximum(0, np.abs(x) - lmbda)\n",
    "\n",
    "def proximal_gradient_method(A, b, lmbda, max_iter=100000, tol=1e-5):\n",
    "    n, m = A.shape\n",
    "    w = np.zeros(m) \n",
    "    alpha = 1.0 / (np.linalg.norm(A.T @ A))\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        gradient = A.T.dot(A.dot(w) - b)\n",
    "        w_old = w.copy()\n",
    "        \n",
    "        w = w - alpha * gradient\n",
    "        \n",
    "        w = soft_thresholding(w, alpha * lmbda)\n",
    "\n",
    "        if np.linalg.norm(w - w_old) < tol:\n",
    "            break\n",
    "    \n",
    "    return w\n",
    "\n",
    "## Unica duvida é se uso test ou train no optimal value\n",
    "result = proximal_gradient_method(A, b, 200)\n",
    "print(\"Result : \", result)\n",
    "print(\"Optimal Value:\", np.linalg.norm(A @ result - b) ** 2 + 200 * np.linalg.norm(result, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dab06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2128d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978b329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c55333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff88e765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb52aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8492806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e161772",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WILLLLLLLLL 4.2\n",
    "\n",
    "# def soft_thresholding(x, threshold):\n",
    "#     return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "# def proximal_gradient_descent(\n",
    "#     max_iterations,\n",
    "#     step_size,\n",
    "#     w_init,\n",
    "#     lambd,\n",
    "#     obj_func,\n",
    "#     grad_func,\n",
    "#     threshold=1e-6,\n",
    "# ):\n",
    "#     w = w_init\n",
    "#     w_history = w\n",
    "#     f_history = obj_func(w)\n",
    "\n",
    "#     for iteration in range(max_iterations):\n",
    "#         w = soft_thresholding(w - step_size * grad_func(w), lambd*step_size)\n",
    "\n",
    "#         w_history = np.vstack((w_history, w))\n",
    "#         f_history = np.vstack((f_history, obj_func(w)))\n",
    "\n",
    "#         if np.absolute(f_history[-1] - f_history[-2]) < threshold:\n",
    "#             break\n",
    "\n",
    "#     return w_history, f_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b48c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "2/np.linalg.norm(A.T@A)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1ec9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46293015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e59beba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66fa143d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9e100e2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
