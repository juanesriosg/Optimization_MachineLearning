{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rapport Maël Olivier et Lina Kaddouri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 / Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 3.1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that \\(Aw = b\\), we can express the components of \\(Aw\\) as:\n",
    "\n",
    "(A w)_t = x(t)^{T} w_1 + w_0 - y(t) x(t)^{T} w_2 = b_t = y(t).\n",
    "\n",
    "\n",
    "Therefore, we have:\n",
    "\n",
    "y(t) = w_1^{T} x(t) + w_0 + w_2^{T} x(t) + 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 3.2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading data\n",
    "\n",
    "data_matrix_train, COP_train, data_matrix_test, COP_test, names = np.load('data_center_data_matrix.npy', allow_pickle=True)\n",
    "\n",
    "# Constructing matrices for min_w ||A w - b||_2**2\n",
    "\n",
    "matrix_mean = np.mean(data_matrix_train, axis=0)\n",
    "M = data_matrix_train - matrix_mean\n",
    "matrix_std = np.std(M, axis=0)\n",
    "M = M / matrix_std\n",
    "\n",
    "A = np.hstack([M, np.ones((M.shape[0],1)), -(M.T * COP_train[:,3]).T])\n",
    "b = COP_train[:,3]\n",
    "\n",
    "w, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "\n",
    "print(\"Solution (w):\", w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 3.3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = np.dot(A_test, w)\n",
    "residuals_test = b_test - predictions_test\n",
    "mse_test = np.mean(residuals_test**2)\n",
    "\n",
    "print(\"Mean Squared Error on Test Set:\", mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 3.4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_train, COP_train, data_matrix_test, COP_test, names = np.load('data_center_data_matrix.npy', allow_pickle=True)\n",
    "\n",
    "matrix_mean = np.mean(data_matrix_train, axis=0)\n",
    "M = data_matrix_train - matrix_mean\n",
    "matrix_std = np.std(M, axis=0)\n",
    "M = M / matrix_std\n",
    "\n",
    "lambda_value = 100\n",
    "A_regularized = np.hstack([M, np.ones((M.shape[0], 1)), -(M.T * COP_train[:, 3]).T])\n",
    "b_regularized = COP_train[:, 3]\n",
    "\n",
    "lambda_matrix = lambda_value * np.eye(A_regularized.shape[1])\n",
    "\n",
    "w_regularized, _, _, _ = np.linalg.lstsq(\n",
    "    np.vstack([A_regularized, lambda_matrix]),\n",
    "    np.concatenate([b_regularized, np.zeros(A_regularized.shape[1])]),\n",
    "    rcond=None\n",
    ")\n",
    "\n",
    "print(\"Regularized Solution (w):\", w_regularized)\n",
    "\n",
    "M_test = (data_matrix_test - matrix_mean) / matrix_std\n",
    "A_test_regularized = np.hstack([M_test, np.ones((M_test.shape[0], 1)), -(M_test.T * COP_test[:, 3]).T])\n",
    "b_test_regularized = COP_test[:, 3]\n",
    "\n",
    "predictions_test_regularized = np.dot(A_test_regularized, w_regularized)\n",
    "residuals_test_regularized = b_test_regularized - predictions_test_regularized\n",
    "mse_test_regularized = np.mean(residuals_test_regularized**2)\n",
    "\n",
    "print(\"Mean Squared Error on Test Set with Regularization:\", mse_test_regularized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 3.5/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le gradient est de la forme A^T( Aw - b) + λw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dérivée seconde est de la forme (A^T)A + λI, qui est bien définie positive (si on multiplie d'un côté par X^T, et de l'autre par X, pour un X quelconque non nul de bonne dimension, on aura la somme de deux normes, ce qui est bien positif ou nul). f_1 est donc bien convexe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 3.6/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_f1(A, w, b, lam):\n",
    "    return A.T @ (A @ w - b) + lam * w\n",
    "\n",
    "def gradient_norm(grad):\n",
    "    return np.linalg.norm(grad)\n",
    "\n",
    "def gradient_descent(A, b, lam, alpha, max_iterations=1000, tolerance=1):\n",
    "    w = np.zeros(A.shape[1])\n",
    "    iterations = 0\n",
    "    \n",
    "    while iterations < max_iterations:\n",
    "        grad = gradient_f1(A, w, b, lam)\n",
    "        w = w - alpha * grad\n",
    "        \n",
    "        if gradient_norm(grad) <= tolerance:\n",
    "            break\n",
    "        \n",
    "        iterations += 1\n",
    "    \n",
    "    return w, iterations\n",
    "\n",
    "lam = 100                    \n",
    "alpha = 0.1                  \n",
    "\n",
    "w_optimal, num_iterations = gradient_descent(A, b, lam, alpha)\n",
    "print(\"Optimal w:\", w_optimal)\n",
    "print(\"Number of iterations:\", num_iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4/ Regularization  for a sparse model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 4.1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut prendre $f_{2}(w)=\\frac{1}{2}\\|A w-b\\|_{2}^{2}$ $g_{2}(w)=\\lambda\\|w\\|_{1}$\n",
    "\n",
    "On a donc: $\\nabla f_{2}(w)=A^{\\top}(A w-b)$ et proxg2 $(v, r)=\\operatorname{sign}(v) \\times(|v|-t)_{+}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 4.2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximal_operator_l1(v, lam, alpha):\n",
    "    return np.sign(v) * np.maximum(np.abs(v) - alpha * lam, 0)\n",
    "\n",
    "def proximal_gradient_method(A, b, lam, alpha, max_iterations=1000, tolerance=1e-6):\n",
    "    w = np.zeros(A.shape[1])\n",
    "    iterations = 0\n",
    "    \n",
    "    while iterations < max_iterations:\n",
    "        gradient = A.T @ (A @ w - b)\n",
    "        w_new = proximal_operator_l1(w - alpha * gradient, lam, alpha)\n",
    "        \n",
    "        if np.linalg.norm(w_new - w, ord=1) < tolerance:\n",
    "            break\n",
    "        \n",
    "        w = w_new\n",
    "        iterations += 1\n",
    "    \n",
    "    return w, iterations\n",
    "\n",
    "lam = 200\n",
    "alpha = 0.01\n",
    "\n",
    "w_optimal, num_iterations = proximal_gradient_method(A, b, lam, alpha)\n",
    "print(\"Optimal w:\", w_optimal)\n",
    "print(\"Number of iterations:\", num_iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
